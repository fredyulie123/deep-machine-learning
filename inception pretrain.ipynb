{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radiomics - 1p/19q Chromosome Status Classification Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special thanks to:**  \n",
    "- Dr. Bradley J. Erickson M.D., Ph.D. - Department of Radiology - Mayo Clinic\n",
    "- Dr. Panagiotis Korfiatis, Ph.D. - Department of Radiology - Mayo Clinic\n",
    "- Dr. Daniel LaChance, M.D. - Department of Neurology - Mayo Clinic\n",
    "\n",
    "**Acknowledgment**\n",
    "National Cancer Institute (NCI), *GrantNo. CA160045* and The Cancer Imaging Archive (TCIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Diagnosis of a brain tumor can be devastating to patients. Surgery is an essential step in management and can be painful and life-threatening. Often the next steps are radiation and chemotherapy which also have side effects and are expensive. Targeted therapies based on genetic properties of the tumor can improve response and reduce the side effects of treatment.  Thanks to work being performed at Mayo Clinic, new approaches using deep learning techniques to detect biomarkers can lead to more effective treatments and yield better health outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Radiogenomics refers to the correlation between cancer imaging features and gene expression. Radiogenomics can be used to identify biomarkers that characterize the genomics of a disease without the use of an invasive biopsy. A biomarker is simply an indicator of some biological state or condition. Four biomarkers that appear important for brain tumors include 1p/19q chromosome co-deletion; MGMT-promoter methylation; IDH-1 mutation; and TERT. The focus of this lab is detection of 1p/19q co-deletion using deep learning, specifically convolutional neural networks. What is remarkable about this research is the novelty and promising results of combining deep learning with Radiogenomics. In addition to avoiding invasive biopsies, the detection or absence of biomarkers is significant because certain treatments of brain tumors are more effective in the presence or absence of a biomarker. Biomarker detection can ensure patients receive the most effective treatment for their particular scenario.\n",
    "\n",
    "Research has been performed demonstrating the detection of 1p/19q co-deletion using deep learning applied to T2 weighted MRI. **The lab instructions that follow will guide you through one approach to establishing your own convolutional neural network to detect 1p/19q co-deletion biomarker. More specifically we will demonstrate how to create a network that, given an MRI scan, can detect the areas of hyperintensity and classify these areas as 1p/19q codeleted or not.** We will design a classifier that differentiates between slices that contain hyperintensities and, for the slices of the exam that contain hyperintensities, decides if the areas are codeleted or not. \n",
    "This eliminates the need for manual tumor delineation during the inference step.\n",
    "You will experiment with alterations to the network architecture, hyperparameters, and training data. We will investigate the use of small and large networks, implement methods to avoid overfitting and overcome class imbalances.  We encourage you to do such experimentation and to utilize deep learning for the detection of other biomarkers and classification tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before going into details on architecting and training a neural network, there are some housekeeping tasks that need to be executed first.  Examples of such tasks include: \n",
    "- Importing libraries\n",
    "- Setting necessary parameters for reproducibility \n",
    "\n",
    "The above list reflects only some of the basic tasks you may need to perform at your site.\n",
    "\n",
    "Go ahead and place your cursor in each of the three code boxes below and click the **\"run cell\" button**, or press **Shift+Enter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load general use libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys, logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "np.random.seed(1337) # for reproducibility\n",
    "from collections import Counter\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Keras libraries \n",
    "\n",
    "We will be using Keras with Tensorflow as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from _util import readfile, FindClass, ValidateModel, plot_confusion_matrix, createROCplots\n",
    "keras.backend.set_floatx('float32')\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Set logging level for the tensorflow library. \n",
    "from genseq import BlockSequence # Data genenrator, This will provide the data to the classifier. \n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU                                   \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sklearn library for evaluation purposes\n",
    "\n",
    "These libraries will help us calculate the evaluation metrics of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import metrics as me\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture \n",
    "The code block that follows defines our network architecture and provides code that visualizes the model's performance.  Notice how easy it is to define a convolution neural network!  Let's take a closer look at some of the layers in the network architecture by exploring the Keras / TensorFlow options used.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Sequential()**\n",
    "\n",
    "Immediately after the def cnn_model line below we see the following code: \n",
    "\n",
    "`model = Sequential()`\n",
    "\n",
    "Sequential() is a Keras specification that allows us to create a linear stack of layers in our neural network architecture.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Conv2D()**\n",
    "\n",
    "`model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform'))`\n",
    "\n",
    "Kernels act like filters.  Each kernel slides over an input image / space to yield a value that projects to a new space (see picture below).  As noted earlier, smaller convolutions offered better results because the smaller convolutions capture some of the finer details of the edges of a tumor.  All convolutional layers in this network use 3 X 3 kernels.  The architecture starts with 16 kernels in a layer, moves to using 32 kernels per layer, then 64 kernels per layer, and reaches a high of 96 kernels per layer.\n",
    "\n",
    "\n",
    "One of the major advantages of using deep neural networks is that you do not have to define the features yourself.  Convolutions find features in the data for you.  Each kernel needs an initial set of weights. There are numerous approaches to initializing these weights - 'he_uniform' was chosen here.  Feel free to experiment using other weight initialization options.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/Convolution.png\" width=\"400\" height=\"300\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**ReLU()**\n",
    "\n",
    "`model.add(ReLU())`\n",
    "\n",
    "ReLU is one of many activation functions (sigmoid, tanh, etc.) that can be selected. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**MaxPooling2D()**\n",
    "\n",
    "`model.add(MaxPooling2D(pool_size=(2, 2)))`\n",
    "\n",
    "Pooling is a down-sampling technique that reduces the number of computations that must be performed.  Maxpooling identifies the most important feature in a specified area (2 X 2 given the example above) and projects the maximum value onto a new space.  This model initially uses 2 X 2 maxpooling and then uses 5 X 5 maxpooling.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Flatten()**\n",
    "\n",
    "`model.add(Flatten())`\n",
    "\n",
    "Flattens the input into a 1-D layer. Does not affect the number of neurons in the layer, just their arrangement.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Dense()**\n",
    "\n",
    "`model.add(Dense(128))`\n",
    "\n",
    "Dense adds a fully connected layer with a fixed number of neurons.  Dense is used twice to decrease the number of neurons to three just before the softmax layer to reflect what will be the binary prediction of either 'no hyperintensity,' 'co-deletion,' or 'no co-deletion'.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**GaussianNoise()**\n",
    "\n",
    "`model.add(keras.layers.noise.GaussianNoise(0.2))`\n",
    "\n",
    "To prevent overfitting of the training data, noise was intentionally added.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Dropout()**\n",
    "\n",
    "`model.add(Dropout(0.25))`\n",
    "\n",
    "Dropout represents yet another means of preventing or limiting overfitting of your model to the training data.  Dropout randomly avoids neurons during the forward and backward propagation phases.  The number of neurons not updating depends upon the value specified inside Dropout().  In this example, one quarter (0.25) of the neurons are skipped.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Activation('softmax')**\n",
    "\n",
    "`model.add(Activation('softmax'))`\n",
    "\n",
    "The last layer in our network, the output layer, uses a softmax activation function to clasify the sample to the correct class 'No hyperintensity','1p/19q co-deletion' and '1p/19q preserved'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Network Architecture Quiz_**\n",
    "\n",
    "<a name=\"Network_Architecture_Questions\"></a>\n",
    "\n",
    "[Network Architecture Quiz Answers](#Network_Architecture_Answers)\n",
    "\n",
    "\n",
    "If you do not know the answer, feel free to search the internet.\n",
    "\n",
    "**Question 1:**  What are the advantages and disadvantages to having fewer layers?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  What are the advantages and disadvantages to having fewer neurons?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**  Why might you choose a smaller kernel size and why might you choose a larger kernel size?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**  Is MaxPooling the only type of pooling?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:**  What does overfitting mean?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a directory to store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"MainExecutable\")\n",
    "logger.info('{}'.format(K.image_data_format()))\n",
    "logger.info('{}'.format(K.backend()))\n",
    "logger.info('{}'.format(str(os.getpid())))\n",
    "directory='/dli/tasks'\n",
    "logger.info('Will place output in the following folder: %s'%(directory))\n",
    "# The following lines of code will create a directory named Results. All our results and models will be placed in this folder. \n",
    "if not os.path.exists(directory):\n",
    "\tos.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the data generator to load the training and validation data\n",
    "\n",
    "Instead of loading the whole data set in memory the data generator will read the data in small parts. In this example we will be utilizing a data generator that reads nifti files. The current generator can be easily modified and adapted to read DICOM files as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Load training/validation')\n",
    "# Parameters\n",
    "batchSize=8\n",
    "sizex=240\n",
    "sizey=240\n",
    "n_labels=3\n",
    "channels=1\n",
    "biom='pq'\n",
    "# Set up data generator\n",
    "TrainSequence = BlockSequence ( dataPath='Train.txt', batchSize=batchSize, sizex=sizex, sizey=sizey,n_labels=n_labels,channels=channels,biom=biom )\n",
    "ValidationSequence = BlockSequence ( dataPath='Validation.txt', batchSize=batchSize, sizex=sizex, sizey=sizey,n_labels=n_labels,channels=channels,biom=biom )\n",
    "logger.info('Loading Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are our data balanced?\n",
    "When designing any type of machine learning system it is useful to know how many samples per class our dataset includes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'No HyperIntensity': 1721, 'Not Codeleted': 628, 'Codeleted': 248})\n"
     ]
    }
   ],
   "source": [
    "with open(\"Train.txt\" ) as f:\n",
    "\t\tPatients = f.read().splitlines()\n",
    "\n",
    "labels_train=[]\n",
    "for subject in Patients:\n",
    "\timagedata,label=readfile(image_ori=subject,channels=1,biom=\"pq\")\n",
    "\tlabels_train.append(label)\n",
    "logger.debug('{}'.format(str(Counter(labels_train))))\n",
    "Summary=Counter(labels_train)\n",
    "Summary['No HyperIntensity'] = Summary.pop(0)\n",
    "Summary['Not Codeleted'] = Summary.pop(1)\n",
    "Summary['Codeleted'] = Summary.pop(2)\n",
    "logger.info('{}'.format(str(Summary)))\n",
    "print ((Summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Through the course of this research multiple hyperparameter values were tested.  The code below represents the combination of hyperparameters and their corresponding values that provided the best results.  When experimenting with this architecture or adapting it to your own data you may want to alter the values shown here to see if you get better results.  Let's take a closer look at some of the hyperparameters and code shown in the next code block.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Batch Size**\n",
    "\n",
    "The batch size indicates the number of images that are processed at one time during forward propagation to yield a loss value that is used in backpropagation.  Typically batch size is set to a power of two and is limited by the amount of memory available.  Also, though larger batch size may allow for faster training, weights update less frequently and may fail to provide the best results. Smaller batch sizes may require smaller learning rates for stability.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Number of Epochs**\n",
    "\n",
    "The number of epochs represents the number of times the entire training dataset is iterated over when training your model.  Validation, performed at the end of each epoch, identifies how well the model generalizes to the validation data (not included in the training process).\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Optimizers and parameters**\n",
    "\n",
    "Choosing the best optimization algorithm can lead to a neural network model with slightly better and faster results. Common options for the optimizer are:\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Adam \n",
    "- RMS\n",
    "\n",
    "**Learning rate**\n",
    "Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect to the loss gradient. The lower the value, the slower we travel along the downward slope. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a value for the leanring rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate=0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to use NADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=keras.optimizers.nadam(lr=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or, run this cell instead to use ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=keras.optimizers.adam(lr=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training monitoring \n",
    "**ReduceLROnPlateau()**\n",
    "\n",
    "`lr_reducer = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)`\n",
    "\n",
    "The learning rate represents how much we move in a certain direction in attempt to find the global minima.  Since initial weight values are somewhat arbitrary, starting with a higher learning rate typically works fine.  As we progress through the training phase, we should get closer and closer to either the global or local minimum.  Since we do not want to overshoot the minimum, a common practice is to anneal the learning rate.  In other words, as we progress through the training phase, we start to take smaller and smaller steps in a certain direction. ReduceLROnPlateau() represents one example of how you can anneal the learning rate.  In the code example above, we will keep reducing the learning rate by the square root of 0.1 when there is no change in the loss value as long as the learning rate has not reached a reduction of 5e-7.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**EarlyStopping()**\n",
    "\n",
    "`early_stopper = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=10)`\n",
    "\n",
    "Early stopping mechanisms help prevent or limit overfitting models to training data.  Also, early stopping mechanisms prevent unnecessary computations when results remain static.  The model presented here will stop training if for a patience of 10 (epochs) there has not been a change of at least 0.001.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**ModelCheckpoint()**\n",
    "\n",
    "`    keras.callbacks.ModelCheckpoint( directory+'/weights.{epoch:04d}-{val_loss:.2f}.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)`\n",
    "\n",
    "ModelCheckpoint will monitor the model and save the weights.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks:\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=30, min_lr=0.0000001, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint( directory+'/weights.{epoch:04d}-{val_loss:.2f}.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1),\n",
    "    keras.callbacks.ModelCheckpoint( directory+'/best.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0.01, patience=100, verbose=0, mode='auto'),\n",
    "    keras.callbacks.TerminateOnNaN(),\n",
    "    keras.callbacks.TensorBoard(log_dir=directory+'/tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Hyperparameter Quiz_**\n",
    "\n",
    "<a name=\"Hyperparameter_Questions\"></a>\n",
    "\n",
    "[Hyperparameter Quiz Answers](#Hyperparameter_Answers)\n",
    "\n",
    "\n",
    "If you do not know the answer, feel free to search the internet.\n",
    "\n",
    "**Question 1:**  Why would you want to increase the batch size?  Identify one possible concern associated with larger batch sizes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  What are two roles early stopping mechanisms play when it comes to building models?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**  Why would you want to increase the number of epochs?  Why would you decrease the number of epochs?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A. Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below provides values for some of the hyperparameters and creates the datasets.  After going through this lab with the pre-defined values, feel free to come back to this code and alter the hyperparameter values to see if you can improve upon the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "### The following cell will create  an Inception model. This model is already implemented in the Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "inception = InceptionV3(include_top=True, weights=None, input_tensor=None, input_shape=(240,240, 1), pooling=None, classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained network to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionpretrain = InceptionV3(include_top=True, weights=None, input_tensor=None, input_shape=(240,240, 1), pooling=None, classes=3)\n",
    "inceptionpretrain.load_weights(\"inception2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some modules\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization, ReLU\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Activation, Dense, Dropout\n",
    "from keras.layers.core import ActivityRegularization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def simpleCNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform',\n",
    "                     input_shape=(240, 240, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(keras.layers.noise.GaussianNoise(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(ReLU())   # add an advanced activation\n",
    "    model.add(Dropout(0.25)) # Avoid over-fitting  \n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "CNN=simpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained network to start\n",
    "**If you modify the above cell, please skip this code cell or comment the following lines.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNpretrain=simpleCNN()\n",
    "CNNpretrain.load_weights(\"CNN2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model and Train\n",
    "The code that follows creates an instance of the previously defined network and will use the previously defined hyperparameter values to compile and fit the model. The fit process will use the previously defined generators to read the data.  Please take a moment to read through the following code before executing as the code reveals several approaches that can be taken in your own research to better understand both the data and the model produced.\n",
    "\n",
    "- Select one of the following options\n",
    "    - Use the Simple CNN with or without the weights \n",
    "    - Use the Inception network with or without the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ONE of the following 4 cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) If you want to use the CNN pretrained network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNNpretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) If you want to use the Inception pretrained network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=inceptionpretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) If you want to use the Inception network  (Train from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) If you want to use the CNN network   (Train from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see how our model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 240, 240, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 119, 119, 32) 288         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 119, 119, 32) 96          conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 119, 119, 32) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 117, 117, 32) 9216        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 117, 117, 32) 96          conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 117, 117, 32) 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 117, 117, 64) 18432       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 117, 117, 64) 192         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 117, 117, 64) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 58, 58, 64)   0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 58, 58, 80)   5120        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 58, 58, 80)   240         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 58, 58, 80)   0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 56, 56, 192)  138240      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 56, 56, 192)  576         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 56, 56, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 27, 27, 192)  0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 27, 27, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 27, 27, 64)   192         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 27, 27, 64)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 27, 27, 48)   9216        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 27, 27, 96)   55296       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 27, 27, 48)   144         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 27, 27, 96)   288         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 27, 27, 48)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 27, 27, 96)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 27, 27, 192)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 27, 27, 64)   12288       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 27, 27, 64)   76800       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 27, 27, 96)   82944       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 27, 27, 32)   6144        average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 27, 27, 64)   192         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 27, 27, 64)   192         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 27, 27, 96)   288         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 27, 27, 32)   96          conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 27, 27, 64)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 27, 27, 64)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 27, 27, 96)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 27, 27, 32)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 27, 27, 256)  0           activation_100[0][0]             \n",
      "                                                                 activation_102[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "                                                                 activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 27, 27, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 27, 27, 64)   192         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 27, 27, 64)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 27, 27, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 27, 27, 96)   55296       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 27, 27, 48)   144         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 27, 27, 96)   288         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 27, 27, 48)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 27, 27, 96)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 27, 27, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 27, 27, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 27, 27, 64)   76800       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 27, 27, 96)   82944       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 27, 27, 64)   16384       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 27, 27, 64)   192         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 27, 27, 64)   192         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 27, 27, 96)   288         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 27, 27, 64)   192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 27, 27, 64)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 27, 27, 64)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 27, 27, 96)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 27, 27, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 27, 27, 288)  0           activation_107[0][0]             \n",
      "                                                                 activation_109[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 27, 27, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 27, 27, 64)   192         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 27, 27, 64)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 27, 27, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 27, 27, 96)   55296       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 27, 27, 48)   144         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 27, 27, 96)   288         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 27, 27, 48)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 27, 27, 96)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 27, 27, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 27, 27, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 27, 27, 64)   76800       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 27, 27, 96)   82944       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 27, 27, 64)   18432       average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 27, 27, 64)   192         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 27, 27, 64)   192         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 27, 27, 96)   288         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 27, 27, 64)   192         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 27, 27, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 27, 27, 64)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 27, 27, 96)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 27, 27, 64)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 27, 27, 288)  0           activation_114[0][0]             \n",
      "                                                                 activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "                                                                 activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 27, 27, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 27, 27, 64)   192         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 27, 27, 64)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 27, 27, 96)   55296       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 27, 27, 96)   288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 27, 27, 96)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 13, 13, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 13, 13, 96)   82944       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 13, 13, 384)  1152        conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 13, 13, 96)   288         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 13, 13, 384)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 13, 13, 96)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 13, 13, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 13, 13, 768)  0           activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 13, 13, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 13, 13, 128)  384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 13, 13, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 13, 13, 128)  114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 13, 13, 128)  384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 13, 13, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 13, 13, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 13, 13, 128)  114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 13, 13, 128)  384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 13, 13, 128)  384         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 13, 13, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 13, 13, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 13, 13, 128)  114688      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 13, 13, 128)  114688      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 13, 13, 128)  384         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 13, 13, 128)  384         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 13, 13, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 13, 13, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 13, 13, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 13, 13, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 13, 13, 192)  172032      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 13, 13, 192)  172032      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 13, 13, 192)  147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 13, 13, 192)  576         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 13, 13, 192)  576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 13, 13, 192)  576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 13, 13, 192)  576         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 13, 13, 192)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 13, 13, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 13, 13, 192)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 13, 13, 192)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 13, 13, 768)  0           activation_125[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 13, 13, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 13, 13, 160)  480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 13, 13, 160)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 13, 13, 160)  179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 13, 13, 160)  480         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 13, 13, 160)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 13, 13, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 13, 13, 160)  179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 13, 13, 160)  480         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 13, 13, 160)  480         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 13, 13, 160)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 13, 13, 160)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 13, 13, 160)  179200      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 13, 13, 160)  179200      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 13, 13, 160)  480         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 13, 13, 160)  480         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 13, 13, 160)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 13, 13, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 13, 13, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 13, 13, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 13, 13, 192)  215040      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 13, 13, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 13, 13, 192)  147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 13, 13, 192)  576         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 13, 13, 192)  576         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 13, 13, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 13, 13, 192)  576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 13, 13, 192)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 13, 13, 192)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 13, 13, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 13, 13, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 13, 13, 768)  0           activation_135[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 13, 13, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 13, 13, 160)  480         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 13, 13, 160)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 13, 13, 160)  179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 13, 13, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 13, 13, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 13, 13, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 13, 13, 160)  179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 13, 13, 160)  480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 13, 13, 160)  480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 13, 13, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 13, 13, 160)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 13, 13, 160)  179200      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 13, 13, 160)  179200      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 13, 13, 160)  480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 13, 13, 160)  480         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 13, 13, 160)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 13, 13, 160)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 13, 13, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 13, 13, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 13, 13, 192)  215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 13, 13, 192)  215040      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 13, 13, 192)  147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 13, 13, 192)  576         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 13, 13, 192)  576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 13, 13, 192)  576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 13, 13, 192)  576         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 13, 13, 192)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 13, 13, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 13, 13, 192)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 13, 13, 192)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 13, 13, 768)  0           activation_145[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 13, 13, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 13, 13, 192)  576         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 13, 13, 192)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 13, 13, 192)  258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 13, 13, 192)  576         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 13, 13, 192)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 13, 13, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 13, 13, 192)  258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 13, 13, 192)  576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 13, 13, 192)  576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 13, 13, 192)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 13, 13, 192)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 13, 13, 192)  258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 13, 13, 192)  258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 13, 13, 192)  576         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 13, 13, 192)  576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 13, 13, 192)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 13, 13, 192)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 13, 13, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 13, 13, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 13, 13, 192)  258048      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 13, 13, 192)  258048      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 13, 13, 192)  147456      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 13, 13, 192)  576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 13, 13, 192)  576         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 13, 13, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 13, 13, 192)  576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 13, 13, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 13, 13, 192)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 13, 13, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 13, 13, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 13, 13, 768)  0           activation_155[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 13, 13, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 13, 13, 192)  576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 13, 13, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 13, 13, 192)  258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 13, 13, 192)  576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 13, 13, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 13, 13, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 13, 13, 192)  258048      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 13, 13, 192)  576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 13, 13, 192)  576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 13, 13, 192)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 13, 13, 192)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 6, 6, 320)    552960      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 6, 6, 192)    331776      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 6, 6, 320)    960         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 6, 6, 192)    576         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 6, 6, 320)    0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 6, 6, 192)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 6, 6, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_166[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "                                                                 max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 6, 6, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 6, 6, 448)    1344        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 6, 6, 448)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 6, 6, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 6, 6, 384)    1548288     activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 6, 6, 384)    1152        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 6, 6, 384)    1152        conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 6, 6, 384)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 6, 6, 384)    0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 6, 6, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 6, 6, 384)    442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 6, 6, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 6, 6, 384)    442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 6, 6, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 6, 6, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 6, 6, 384)    1152        conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 6, 6, 384)    1152        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 6, 6, 384)    1152        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 6, 6, 384)    1152        conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 6, 6, 192)    245760      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 6, 6, 320)    960         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 6, 6, 384)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 6, 6, 384)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 6, 6, 384)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 6, 6, 384)    0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 6, 6, 192)    576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 6, 6, 320)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_173[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 6, 6, 768)    0           activation_177[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 6, 6, 192)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_171[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 6, 6, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 6, 6, 448)    1344        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 6, 6, 448)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 6, 6, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 6, 6, 384)    1548288     activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 6, 6, 384)    1152        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 6, 6, 384)    1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 6, 6, 384)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 6, 6, 384)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 6, 6, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 6, 6, 384)    442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 6, 6, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 6, 6, 384)    442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 6, 6, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 6, 6, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 6, 6, 384)    1152        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 6, 6, 384)    1152        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 6, 6, 384)    1152        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 6, 6, 384)    1152        conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 6, 6, 192)    393216      average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 6, 6, 320)    960         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 6, 6, 384)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 6, 6, 384)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 6, 6, 384)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 6, 6, 384)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 6, 6, 192)    576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 6, 6, 320)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_182[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6, 6, 768)    0           activation_186[0][0]             \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 6, 6, 192)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_180[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 3)            6147        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,808,355\n",
      "Trainable params: 21,773,923\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "class_weight= {0 : 1., 1:3., 2:5.}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "325/325 [==============================] - 48s 147ms/step - loss: 0.2190 - acc: 0.9619 - val_loss: 0.1140 - val_acc: 0.9618\n",
      "Epoch 2/2\n",
      "325/325 [==============================] - 34s 105ms/step - loss: 0.0312 - acc: 0.9954 - val_loss: 0.1027 - val_acc: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f434dfac9b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Class imbalance\n",
    "# Fit\n",
    "model.fit_generator(TrainSequence,steps_per_epoch=len(TrainSequence) ,validation_data = ValidationSequence,validation_steps = len(ValidationSequence) \n",
    "                              ,epochs=epochs,workers=1, max_queue_size=4,verbose=1,callbacks=callbacks, shuffle=False,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model on validation set \n",
    "We will calculate:\n",
    "\n",
    "- the confusion matrix\n",
    "- Performance metrics (Precision, recall, f1-score)\n",
    "- ROC curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predictedoutput, truth=ValidateModel(model,\"Validation.txt\")\n",
    "print('Confusion matrix: Validation')\n",
    "cm=(confusion_matrix(truth, predicted_class))        \n",
    "target_names=['No Hyper Intensity', '1p19q preserved', '1p19q codeleted' ]\n",
    "plot_confusion_matrix(cm, target_names,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix - Validation',\n",
    "                          cmap=plt.cm.Blues)\n",
    "print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "print(me.classification_report( truth, predicted_class, target_names=target_names,digits=4))\n",
    "\n",
    "print ('Summary')\n",
    "print (\"Precision: \", (me.precision_score( truth, predicted_class,average='weighted')))\n",
    "print (\"Recall: \", (me.recall_score( truth, predicted_class,average='weighted')))\n",
    "print (\"F1 Score: \", me.f1_score( truth, predicted_class,average='weighted'))\n",
    "\n",
    "\n",
    "createROCplots(truth, predictedoutput )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model on test set \n",
    "We will calculate:\n",
    "\n",
    "- the confusion matrix\n",
    "- Performance metrics (Precission, recall, f1-score)\n",
    "- ROC curves\n",
    "It is important to note that the performance during the validation and testing phase must not differ significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predictedoutput, truth=ValidateModel(model,\"Test.txt\")\n",
    "print('Confusion matrix: Test')\n",
    "cm=(confusion_matrix(truth, predicted_class))        \n",
    "target_names=['No Hyper Intensity', '1p19q preserved', '1p19q codeleted' ]\n",
    "plot_confusion_matrix(cm, target_names,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix - Validation',\n",
    "                          cmap=plt.cm.Blues)\n",
    "print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "\n",
    "print(me.classification_report( truth, predicted_class, target_names=target_names,digits=4))\n",
    "print (\"Precision: \", (me.precision_score( truth, predicted_class,average='weighted')))\n",
    "print (\"Recall: \", (me.recall_score( truth, predicted_class,average='weighted')))\n",
    "print (\"F1 Score: \", me.f1_score( truth, predicted_class,average='weighted'))\n",
    "\n",
    "\n",
    "createROCplots(truth, predictedoutput )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you do better? \n",
    "\n",
    "Try to create your own model and run cross validation \n",
    "\n",
    "\n",
    " - Add/remove layers<br>\n",
    "     For example:\n",
    "     - model.add(Conv2D(8, (3, 3)))\n",
    "     - model.add(Conv2D(16, (3, 3),activation='tanh',kernel_initializer='he_uniform')\n",
    "     * Adjust maxpooling accordingly \n",
    "     [extra help](https://keras.io/layers/core/)\n",
    "     \n",
    "     \n",
    " - Try different optimizers <br>\n",
    "     For example: Adam, SGD \n",
    "     - altoptimizer=keras.optimizers.SGD(lr=0.0001)\n",
    "     - altoptimizer=keras.optimizers.Adam(lr=0.0001)\n",
    "         - Change the learning rate:\n",
    "you can choose values between .1 to 2e-6\n",
    "     [extra help](https://keras.io/optimizers/)\n",
    "            \n",
    "     \n",
    " - Change convolution size <br>\n",
    "     For example:\n",
    "     - model.add(Conv2D(8, (5, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Apply the model on a T2 weighted scan\n",
    "Once you believe you have captured the best model, then it is time to evaluate a final set of results using all the data to ensure that the model's precision, recall, and accuracy are at their optimum on the test set.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a nifti file\n",
    "testcase='TESTCASE1_trueclass1.nii.gz'\n",
    "\n",
    "# testcase='TESTCASE2_trueclass1.nii.gz'\n",
    "\n",
    "imagedata,label=readfile(image_ori=testcase,channels=1,biom=\"trueclass\", test=True)\n",
    "\n",
    "print ('The correct class is ', FindClass(testcase, start='trueclass', end='.nii.gz'))\n",
    "# Load the model \n",
    "loaded_model = load_model(directory+'/best.h5')\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "# Change the shape of the array to much the one our classifier requires\n",
    "imagedata=np.swapaxes(imagedata,2,1)\n",
    "imagedata=np.swapaxes(imagedata,1,0)\n",
    "# Apply the classifier \n",
    "imagedata=imagedata.reshape(-1,240,240,1)\n",
    "prediction = loaded_model.predict(imagedata)\n",
    "# print ('-----')\n",
    "prediction= np.argmax(prediction, axis=1)\n",
    "# print ('prediction')\n",
    "# print (prediction)\n",
    "# print ('---')\n",
    "plt.figure(figsize=(50,50))    \n",
    "number_of_subplots=np.shape(imagedata)[0]\n",
    "\n",
    "for i,v in enumerate(range(number_of_subplots)):\n",
    "    v = v+1\n",
    "    ax1 = plt.subplot(number_of_subplots,1,v)\n",
    "    ax1.set_title(\"Predicted Class is: \"+str(prediction[i]))\n",
    "    ax1.imshow(np.rot90(imagedata[i,:,:,0]), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What Next\n",
    "Many ways to explore and possibly improve the model:\n",
    "\n",
    "- Add additional layers to the network\n",
    "\n",
    "- Change the number of neurons in those layers\n",
    "\n",
    "- Change some of the hyperparameters in the network configuration like dropout or learning rate, etc.\n",
    "\n",
    "\n",
    "Time permitting, go back to the code above and experiment with some of these suggested changes to see if you can achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture Quiz Answers\n",
    "\n",
    "<a name=\"Network_Architecture_Answers\"></a>\n",
    "\n",
    "[Network Architecture Quiz Questions](#Network_Architecture_Questions)\n",
    "\n",
    "**Question 1:**  What are the advantages and disadvantages to having fewer layers?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Fewer layers have fewer computations that must be performed which leads to faster training times.  However, it may also prevent the discovery of higher and higher level features that leads to more accurate models if more layers were included in the network architecture.\n",
    "<br>\n",
    "<br>\n",
    "**Question 2:**  What are the advantages and disadvantages to having fewer neurons?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Fewer neurons means that you have fewer features.  Fewer features may yield models with lower than expected accuracy levels.  Alternatively, if a smaller number of features yields acceptable levels of accuracy, then the reduced number of computations should yield faster training times.\n",
    "<br>\n",
    "<br>\n",
    "**Question 3:**  Why might you choose a smaller kernel size and why might you choose a larger kernel size?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Smaller kernel sizes may be better at capturing some of the finer details of an image that would otherwise be lost if larger kernel sizes were selected.  This could lead to a model that does not reach an acceptable level of accuracy.  Alternatively, larger kernel sizes reduce the number of computations being performed; thus, leading to shorter training times.  If a higher degree of information loss can be afforded, using larger kernel sizes may be desirable given the reduction in training time.  \n",
    "<br>\n",
    "<br>\n",
    "**Question 4:**  Is MaxPooling the only type of pooling?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  There are other types of pooling such as Average Pooling, but MaxPooling is the most commonly used type of pooling.\n",
    "<br>\n",
    "<br>\n",
    "**Question 5:**  What does overfitting mean?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Overfitting occurs when the features identified during the training process are so specific to the training data that the resulting model does not generalize well to new data where the features may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Quiz Answers\n",
    "\n",
    "<a name=\"Hyperparameter_Answers\"></a>\n",
    "\n",
    "[Hyperparameter Quiz Questions](#Hyperparameter_Questions)\n",
    "\n",
    "\n",
    "**Question 1:**  Why would you want to increase the batch size?  Identify one possible concern associated with larger batch sizes.\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Larger batch sizes result in faster training because backpropagation does not take place as often.  Researchers typically set batch size to the maximum number of images that can be held in memory without generating an \"Out of Memory\" error message.  Also, it is common to have the batch size set to a value that is a power of 2, but this is not a requirement.  The possible drawback to a larger batch size is that the weights are not updated as frequently which may result in a model that is not as accurate.  However, it may also mean that model is less likely to be overfitted to the training data.\n",
    "<br>\n",
    "<br>\n",
    "**Question 2:**  What are two roles early stopping mechanisms play when it comes to building models?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Early stopping mechanisms, where training ceases prematurely, can help limit or prevent overfitting the model to the training data and can also help conserve GPU resources.\n",
    "<br>\n",
    "<br>\n",
    "**Question 3:**  Why would you want to increase the number of epochs?  Why would you decrease the number of epochs?\n",
    "<br>\n",
    "<br>\n",
    "**_Answer:_**  Increasing the number of epochs may improve the accuracy of the model.  However, it may also lead to overfitting the model to the training data.  Experimentation with this hyperparameter along with the use of early stopping mechanisms are typical approaches to getting to an acceptable epoch value. Reviewing the loss curves during validation phase c help as well.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
